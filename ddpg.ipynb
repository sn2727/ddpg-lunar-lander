{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG \n",
    "\n",
    "This notebook contains an implementation of Deep Deterministic Policy Gradients (DDPG) which is a model-free reinforcement learning algorithm designed for continuous action spaces, utilizing an actor-critic architecture. \n",
    "In the following DDPG is used to safely land a lunarlander from [Gymnasium environments](https://gymnasium.farama.org/environments/box2d/lunar_lander/).\n",
    "\n",
    "<img src=\"./resources/lunar_lander.gif\" alt=\"Gymnasium\" width=\"20%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install swig\n",
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[box2d]\"\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from collections import namedtuple\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "Remember that for Q-learning, we have the following update:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma \\max_{a'}Q(s', a') - Q(s, a))$\n",
    "\n",
    "with discount factor $\\gamma$, learning rate $\\alpha$, reward $r$, sampled state $s$, sampled next state $s'$ and sampled action $a$. The $\\max$ operator enumerates through the finite number of discrete actions and chooses the action correcponding to the maximum $Q(s', a')$. The $\\max$ operator essentially represents the greedy policy for a discrete case. \n",
    "\n",
    "In continuous action settings there are infinite many actions and hence we need a new strategy for selecting actions. This is where the Actor network, $\\mu(s)$ comes into play. The actor is the explicit representation of the action selection strategy (or policy). It takes in a state $s$ as input and outputs an action $a$ which it believes will yield the highest $Q$ value in state $s$. (Do you see the resembelance to the $\\max$ operator in DQN?)\n",
    "\n",
    "So we can rewrite the Q-learning update with the actor:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s', \\mu(s')) - Q(s, a))$\n",
    "\n",
    "\n",
    "The critic can then be updated with the Meas Square Loss as before:\n",
    "\n",
    "$ L(\\theta^{Q}) = \\mathop{\\mathbb{E}}_{(s, a, r, s')\\sim D}([r + \\gamma Q(s', \\mu(s';\\theta^{\\mu}); \\theta^{Q}) - Q(s, a; \\theta^{Q})]^2 )$  \n",
    "\n",
    "where D is a dataset of sampled transitions and $\\theta^{Q}$ and $\\theta^{\\mu}$ are parameters of the critic and actor networks respectively.\n",
    "\n",
    "The Actor network's objective is to output actions that maximize the value of the Critic. Hence the loss function for the Actor is the negative of its objective which is the value of the Critic.\n",
    "\n",
    "$L(\\theta^{\\mu}) =  -\\mathop{\\mathbb{E}}_{(s)\\sim D}(Q(s, \\mu(s;\\theta^{\\mu}); \\theta^{Q}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic Network  \n",
    "\n",
    "Critic takes in both (state-based) observation and action and outputs the Q value. \n",
    "\n",
    "The architecture consists of Linear layers and ReLu activations:\n",
    "- fc1: input:(obs_dim + action_dim),  ouput: 256\n",
    "- fc2: input: 256, output: 256\n",
    "- fc3: input: 256, output: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int):\n",
    "        \"\"\"\n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([obs, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Network\n",
    "\n",
    "Actor takes in an observation and outputs an action.\n",
    "\n",
    "The architecture consists of Linear layers and ReLU activations:\n",
    "\n",
    "- fc1: input:(obs_dim),  ouput: 256\n",
    "- fc2: input: 256, output: 256\n",
    "- fc3: input: 256, output: action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, action_low: np.array, action_high: np.array):\n",
    "        \"\"\"\n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((action_high - action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((action_high + action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(obs_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        raw_actions = self.fc3(x)\n",
    "\n",
    "        # Apply tanh activation to squash the actions to the range [-1, 1]\n",
    "        scaled_actions = torch.tanh(raw_actions)\n",
    "\n",
    "        # Scale and translate the actions to match the environment's action space\n",
    "        actions = scaled_actions * self.action_scale + self.action_bias\n",
    "\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer stores transitions of the form $(s, a, r, s')$ with $s$ as the current state, the action $a$, the reward $r$, and the next state $s'$. The buffer can perform two operations:\n",
    "- **store**: During sampling we observe transitions and store them with ``buffer.store(...)``. However, the buffer only has a fixed size\n",
    "(as we cannot store an infinte amount of data). When reaching it, the oldest samples are overwritten first.\n",
    "- **sample**: For training, we want to sample a batch of transitions from our buffer via ``buffer.sample(...)``. The transitions are sampled uniformly and with replacement i.e. the same transition can be sampled more than once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        Create the replay buffer.\n",
    "\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
    "\n",
    "        :param obs: The current observation.\n",
    "        :param action: The action.\n",
    "        :param reward: The reward.\n",
    "        :param next_obs: The next observation.\n",
    "        :param terminated: Whether the episode terminated.\n",
    "        \"\"\"\n",
    "        if len(self.data) < self.max_size:\n",
    "            self.data.append((obs, action, reward, next_obs, terminated))\n",
    "        else:\n",
    "            self.data[self.position] = (obs, action, reward, next_obs, terminated)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
    "\n",
    "        :param batch_size: The batch size.\n",
    "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
    "        \"\"\"\n",
    "\n",
    "        obss, acts, rews, next_obss, terms = [], [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.data))\n",
    "\n",
    "            o, a, r, no, t = self.data[index]\n",
    "            obss.append(torch.as_tensor(o))\n",
    "            acts.append(torch.as_tensor(a))\n",
    "            rews.append(torch.as_tensor(r))\n",
    "            next_obss.append(torch.as_tensor(no))\n",
    "            terms.append(torch.as_tensor(t))\n",
    "\n",
    "        # Stack\n",
    "        obs_batch = torch.stack(obss)\n",
    "        action_batch = torch.stack(acts)\n",
    "        reward_batch = torch.tensor(rews)\n",
    "        next_obs_batch = torch.stack(next_obss)\n",
    "        terminated_batch = torch.tensor(terms)\n",
    "\n",
    "        return obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Update\n",
    "\n",
    "Implementation of update methods for actor and critic network.\n",
    "``update_critic`` and ``update_actor`` get the sampled data from the replay buffer, calculate the critic and actor loss and perform an update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(\n",
    "        critic: nn.Module,\n",
    "        critic_target: nn.Module,\n",
    "        critic_optimizer: optim.Optimizer,\n",
    "        actor_target: nn.Module,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the DDPG's Critic network for one optimizer step.\n",
    "\n",
    "    :param critic: The critic network.\n",
    "    :param critic_target: The target critic network.\n",
    "    :param critic_optimizer: The critic's optimizer.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_target: The target actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param gamma: The discount factor.\n",
    "    :param obs: Batch of current observations.\n",
    "    :param act: Batch of actions.\n",
    "    :param rew: Batch of rewards.\n",
    "    :param next_obs: Batch of next observations.\n",
    "    :param tm: Batch of termination flags.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the target Q-values using the target critic network\n",
    "    next_actions = actor_target(next_obs)\n",
    "    next_q_values = critic_target(next_obs, next_actions)\n",
    "    target_q_values = rew + gamma * (1 - tm.float()) * next_q_values\n",
    "\n",
    "    # Compute the predicted Q-values using the critic network\n",
    "    predicted_q_values = critic(obs, act)\n",
    "\n",
    "    # Compute the critic loss (Mean Squared Bellman Error)\n",
    "    critic_loss = nn.MSELoss()(predicted_q_values.float(), target_q_values.float().detach())\n",
    "\n",
    "    # Perform a gradient descent step\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "def update_actor(critic: nn.Module,\n",
    "                 actor: nn.Module,\n",
    "                 actor_optimizer: optim.Optimizer,\n",
    "                 obs: torch.Tensor,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Update the DDPG's Actor network for one optimizer step.\n",
    "\n",
    "    :param critic: The critic network.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param obs: Batch of current observations.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the actor loss as the negative of the predicted Q-values\n",
    "    actor_loss = -critic(obs, actor(obs)).mean()\n",
    "\n",
    "    # Perform a gradient ascent step\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyak Update of the target networks\n",
    "It is common to update the target networks very slowly at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(\n",
    "    params: Iterable[torch.Tensor],\n",
    "    target_params: Iterable[torch.Tensor],\n",
    "    tau: float,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
    "\n",
    "    :param params: parameters of the original network (model.parameters())\n",
    "    :param target_params: parameters of the target network (model_target.parameters())\n",
    "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) 1 -> Hard update, 0 -> No update\n",
    "    \"\"\"\n",
    "    for target_param, source_param in zip(target_params, params):\n",
    "        target_param.data.mul_(1.0 - tau)\n",
    "        torch.add(source_param.data, alpha=tau, other=target_param.data, out=target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent\n",
    "The DDPG Agent creates critic, actor and target networks, optimizers and implements the train method.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self,\n",
    "            env,\n",
    "            exploration_noise=0.1,\n",
    "            gamma=0.99,\n",
    "            lr=0.001,\n",
    "            batch_size=64,\n",
    "            tau=0.005,\n",
    "            maxlen=100_000,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the DDPG agent.\n",
    "\n",
    "        :param env: The environment.\n",
    "        :param exploration_noise.\n",
    "        :param gamma: The discount factor.\n",
    "        :param lr: The learning rate.\n",
    "        :param batch_size: Mini batch size.\n",
    "        :param tau: Polyak update coefficient.\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "\n",
    "        # Initialize the Replay Buffer\n",
    "        self.replay_buffer = ReplayBuffer(max_size=maxlen)\n",
    "\n",
    "        # Initialize the Critic and Actor networks\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.critic = Critic(obs_dim=obs_dim, action_dim=action_dim)\n",
    "        self.actor = Actor(obs_dim=obs_dim, action_dim=action_dim, action_low=env.action_space.low, action_high=env.action_space.high)\n",
    "\n",
    "        # Initialze the target Critic and Actor networks and load the corresponding state_dicts\n",
    "        self.critic_target = Critic(obs_dim=obs_dim, action_dim=action_dim)\n",
    "        self.actor_target = Actor(obs_dim=obs_dim, action_dim=action_dim, action_low=env.action_space.low, action_high=env.action_space.high)\n",
    "\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # Create ADAM optimizer for the Critic and Actor networks\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, num_episodes: int) -> EpisodeStats:\n",
    "        \"\"\"\n",
    "        Train the DDPG agent.\n",
    "\n",
    "        :param num_episodes: Number of episodes to train.\n",
    "        :returns: The episode statistics.\n",
    "        \"\"\"\n",
    "        # Keeps track of useful statistics\n",
    "        stats = EpisodeStats(\n",
    "            episode_lengths=np.zeros(num_episodes),\n",
    "            episode_rewards=np.zeros(num_episodes),\n",
    "        )\n",
    "        current_timestep = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Print out which episode we're on, useful for debugging.\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}')\n",
    "\n",
    "            # Reset the environment and get initial observation\n",
    "            obs, _ = self.env.reset()\n",
    "\n",
    "            for episode_time in itertools.count():\n",
    "                # Use the Actor to choose an action\n",
    "                action = self.actor_target(torch.as_tensor(obs)).detach().numpy()\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Update statistics\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                stats.episode_lengths[i_episode] += 1\n",
    "\n",
    "                # Store sample in the replay buffer\n",
    "                self.replay_buffer.store(obs, action, reward, next_obs, terminated)\n",
    "\n",
    "                # Sample a mini batch from the replay buffer\n",
    "                # A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch)\n",
    "                samples = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "                # Update the Critic network\n",
    "                update_critic(self.critic, self.critic_target, self.critic_optimizer, self.actor_target, self.gamma,\n",
    "                              samples[0], samples[1], samples[2], samples[3], samples[4])\n",
    "\n",
    "                # Update the Actor network\n",
    "                update_actor(self.critic, self.actor, self.actor_optimizer, samples[0])\n",
    "\n",
    "                # Update the target networks (Critic and Actor) via Polyak Update\n",
    "                polyak_update(self.critic.parameters(), self.critic_target.parameters(), self.tau)\n",
    "                polyak_update(self.actor.parameters(), self.actor_target.parameters(), self.tau)\n",
    "\n",
    "                current_timestep += 1\n",
    "\n",
    "                # Check whether the episode is finished\n",
    "                if terminated or truncated or episode_time >= 500:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now, let's run our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose environment\n",
    "env = gym.make(\"LunarLander-v2\", continuous = True, render_mode=\"rgb_array\")\n",
    "# Print observation and action space infos\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "\n",
    "# Hyperparameters:\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "TAU = 0.005\n",
    "EXPLORATION_NOISE=0.1\n",
    "NUM_EPISODES = 1_000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# Train DDPG\n",
    "agent = DDPGAgent(\n",
    "    env,\n",
    "    exploration_noise=EXPLORATION_NOISE,\n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    tau=TAU,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    ")\n",
    "stats = agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving and loading the trained actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained actor\n",
    "torch.save(agent.actor, \"resources/ddpg_actor.pt\")\n",
    "\n",
    "# loading the trained actor\n",
    "loaded_actor = torch.load(\"resources/ddpg_actor.pt\")\n",
    "loaded_actor.eval()\n",
    "print(loaded_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window=20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\")\n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array).astype(np.uint8)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            action = policy(torch.as_tensor(obs, dtype=torch.float32)).cpu().numpy()\n",
    "\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "imgs = rendered_rollout(loaded_actor, env)\n",
    "save_rgb_animation(imgs, \"resources/trained.gif\")\n",
    "IImage(filename=\"resources/trained.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
