{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG \n",
    "\n",
    "This notebook contains an implementation of Deep Deterministic Policy Gradients (DDPG) which is a model-free reinforcement learning algorithm designed for continuous action spaces, utilizing an actor-critic architecture. \n",
    "In the following DDPG is used to safely land a lunarlander from [Gymnasium environments](https://gymnasium.farama.org/environments/box2d/lunar_lander/).\n",
    "\n",
    "<img src=\"./resources/lunar_lander.gif\" alt=\"Gymnasium\" width=\"20%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: swig in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (4.1.1.post1)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium[box2d]) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium[box2d]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium[box2d]) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pygame>=2.1.3 (from gymnasium[box2d])\n",
      "  Using cached pygame-2.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from gymnasium[box2d]) (4.1.1.post1)\n",
      "Using cached pygame-2.5.2-cp311-cp311-win_amd64.whl (10.8 MB)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [28 lines of output]\n",
      "      Using setuptools (version 68.2.2).\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-311\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-311\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-311\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-311\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "      Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "      Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "      Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "      Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "      Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "      Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "      Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "      Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "ERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: imageio in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (2.33.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from imageio) (1.26.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\simon\\downloads\\deeprl\\.conda\\lib\\site-packages (from imageio) (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install swig\n",
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[box2d]\"\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from collections import namedtuple\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "Remember that for Q-learning, we have the following update:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma \\max_{a'}Q(s', a') - Q(s, a))$\n",
    "\n",
    "with discount factor $\\gamma$, learning rate $\\alpha$, reward $r$, sampled state $s$, sampled next state $s'$ and sampled action $a$. The $\\max$ operator enumerates through the finite number of discrete actions and chooses the action correcponding to the maximum $Q(s', a')$. The $\\max$ operator essentially represents the greedy policy for a discrete case. \n",
    "\n",
    "In continuous action settings there are infinite many actions and hence we need a new strategy for selecting actions. This is where the Actor network, $\\mu(s)$ comes into play. The actor is the explicit representation of the action selection strategy (or policy). It takes in a state $s$ as input and outputs an action $a$ which it believes will yield the highest $Q$ value in state $s$. (Do you see the resembelance to the $\\max$ operator in DQN?)\n",
    "\n",
    "So we can rewrite the Q-learning update with the actor:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s', \\mu(s')) - Q(s, a))$\n",
    "\n",
    "\n",
    "The critic can then be updated with the Meas Square Loss as before:\n",
    "\n",
    "$ L(\\theta^{Q}) = \\mathop{\\mathbb{E}}_{(s, a, r, s')\\sim D}([r + \\gamma Q(s', \\mu(s';\\theta^{\\mu}); \\theta^{Q}) - Q(s, a; \\theta^{Q})]^2 )$  \n",
    "\n",
    "where D is a dataset of sampled transitions and $\\theta^{Q}$ and $\\theta^{\\mu}$ are parameters of the critic and actor networks respectively.\n",
    "\n",
    "The Actor network's objective is to output actions that maximise the value of the Critic. Hence the loss function for the Actor is the negative of its objective which is the value of the Critic.\n",
    "\n",
    "$L(\\theta^{\\mu}) =  -\\mathop{\\mathbb{E}}_{(s)\\sim D}(Q(s, \\mu(s;\\theta^{\\mu}); \\theta^{Q}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic Network  \n",
    "\n",
    "Critic takes in both (state-based) observation and action and outputs the Q value. \n",
    "\n",
    "The architecture consists of Linear layers and ReLu activations:\n",
    "- fc1: input:(obs_dim + action_dim),  ouput: 256\n",
    "- fc2: input: 256, output: 256\n",
    "- fc3: input: 256, output: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int):\n",
    "        \"\"\"\n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([obs, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Network\n",
    "\n",
    "Actor takes in an observation and outputs an action.\n",
    "\n",
    "The architecture consists of Linear layers and ReLU activations:\n",
    "\n",
    "- fc1: input:(obs_dim),  ouput: 256\n",
    "- fc2: input: 256, output: 256\n",
    "- fc3: input: 256, output: action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, action_low: np.array, action_high: np.array):\n",
    "        \"\"\"\n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((action_high - action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((action_high + action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(obs_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(obs))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        raw_actions = self.fc3(x)\n",
    "\n",
    "        # Apply tanh activation to squash the actions to the range [-1, 1]\n",
    "        scaled_actions = torch.tanh(raw_actions)\n",
    "\n",
    "        # Scale and translate the actions to match the environment's action space\n",
    "        actions = scaled_actions * self.action_scale + self.action_bias\n",
    "\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer stores transitions of the form $(s, a, r, s')$ with $s$ as the current state, the action $a$, the reward $r$, and the next state $s'$. The buffer can perform two operations:\n",
    "- **store**: During sampling we observe transitions and store them with ``buffer.store(...)``. However, the buffer only has a fixed size\n",
    "(as we cannot store an infinte amount of data). When reaching it, the oldest samples are overwritten first.\n",
    "- **sample**: For training, we want to sample a batch of transitions from our buffer via ``buffer.sample(...)``. The transitions are sampled uniformly and with replacement i.e. the same transition can be sampled more than once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        Create the replay buffer.\n",
    "\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
    "\n",
    "        :param obs: The current observation.\n",
    "        :param action: The action.\n",
    "        :param reward: The reward.\n",
    "        :param next_obs: The next observation.\n",
    "        :param terminated: Whether the episode terminated.\n",
    "        \"\"\"\n",
    "        if len(self.data) < self.max_size:\n",
    "            self.data.append((obs, action, reward, next_obs, terminated))\n",
    "        else:\n",
    "            self.data[self.position] = (obs, action, reward, next_obs, terminated)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
    "\n",
    "        :param batch_size: The batch size.\n",
    "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
    "        \"\"\"\n",
    "\n",
    "        obss, acts, rews, next_obss, terms = [], [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.data))\n",
    "\n",
    "            o, a, r, no, t = self.data[index]\n",
    "            obss.append(torch.as_tensor(o))\n",
    "            acts.append(torch.as_tensor(a))\n",
    "            rews.append(torch.as_tensor(r))\n",
    "            next_obss.append(torch.as_tensor(no))\n",
    "            terms.append(torch.as_tensor(t))\n",
    "\n",
    "        # Stack\n",
    "        obs_batch = torch.stack(obss)\n",
    "        action_batch = torch.stack(acts)\n",
    "        reward_batch = torch.tensor(rews)\n",
    "        next_obs_batch = torch.stack(next_obss)\n",
    "        terminated_batch = torch.tensor(terms)\n",
    "\n",
    "        return obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Update\n",
    "\n",
    "Implementation of update methods for actor and critic network.\n",
    "``update_critic`` and ``update_actor`` get the sampled data from the replay buffer, calculate the critic and actor loss and perform an update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(\n",
    "        critic: nn.Module,\n",
    "        critic_target: nn.Module,\n",
    "        critic_optimizer: optim.Optimizer,\n",
    "        actor_target: nn.Module,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the DDPG's Critic network for one optimizer step.\n",
    "\n",
    "    :param critic: The critic network.\n",
    "    :param critic_target: The target critic network.\n",
    "    :param critic_optimizer: The critic's optimizer.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_target: The target actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param gamma: The discount factor.\n",
    "    :param obs: Batch of current observations.\n",
    "    :param act: Batch of actions.\n",
    "    :param rew: Batch of rewards.\n",
    "    :param next_obs: Batch of next observations.\n",
    "    :param tm: Batch of termination flags.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the target Q-values using the target critic network\n",
    "    next_actions = actor_target(next_obs)\n",
    "    next_q_values = critic_target(next_obs, next_actions)\n",
    "    target_q_values = rew + gamma * (1 - tm) * next_q_values\n",
    "\n",
    "    # Compute the predicted Q-values using the critic network\n",
    "    predicted_q_values = critic(obs, act)\n",
    "\n",
    "    # Compute the critic loss (Mean Squared Bellman Error)\n",
    "    critic_loss = nn.MSELoss()(predicted_q_values, target_q_values.detach())\n",
    "\n",
    "    # Perform a gradient descent step\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "\n",
    "def update_actor(critic: nn.Module,\n",
    "                 actor: nn.Module,\n",
    "                 actor_optimizer: optim.Optimizer,\n",
    "                 obs: torch.Tensor,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Update the DDPG's Actor network for one optimizer step.\n",
    "\n",
    "    :param critic: The critic network.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param obs: Batch of current observations.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the actor loss as the negative of the predicted Q-values\n",
    "    actor_loss = -critic(obs, actor(obs)).mean()\n",
    "\n",
    "    # Perform a gradient ascent step\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyak Update of the target networks\n",
    "It is common to update the target networks very slowly at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(\n",
    "    params: Iterable[torch.Tensor],\n",
    "    target_params: Iterable[torch.Tensor],\n",
    "    tau: float,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
    "\n",
    "    :param params: parameters of the original network (model.parameters())\n",
    "    :param target_params: parameters of the target network (model_target.parameters())\n",
    "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) 1 -> Hard update, 0 -> No update\n",
    "    \"\"\"\n",
    "    for target_param, source_param in zip(target_params, params):\n",
    "        target_param.data.mul_(1.0 - tau)\n",
    "        torch.add(source_param.data, alpha=tau, other=target_param.data, out=target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent\n",
    "The DDPG Agent creates critic, actor and target networks, optimizers and implements the train method.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self,\n",
    "            env,\n",
    "            exploration_noise=0.1,\n",
    "            gamma=0.99,\n",
    "            lr=0.001,\n",
    "            batch_size=64,\n",
    "            tau=0.005,\n",
    "            maxlen=100_000,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the DDPG agent.\n",
    "\n",
    "        :param env: The environment.\n",
    "        :param exploration_noise.\n",
    "        :param gamma: The discount factor.\n",
    "        :param lr: The learning rate.\n",
    "        :param batch_size: Mini batch size.\n",
    "        :param tau: Polyak update coefficient.\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "\n",
    "        # Initialize the Replay Buffer\n",
    "        self.replay_buffer = ReplayBuffer(max_size=maxlen)\n",
    "\n",
    "        # Initialize the Critic and Actor networks\n",
    "        obs_dim = env.observation_space.shape\n",
    "        action_dim = env.action_space.shape\n",
    "        self.critic = Critic(obs_dim=obs_dim, action_dim=action_dim)\n",
    "        self.actor = Actor(obs_dim=obs_dim, action_dim=action_dim, action_low=env.action_space.low, action_high=env.action_space.high)\n",
    "\n",
    "        # Initialze the target Critic and Actor networks and load the corresponding state_dicts\n",
    "        self.critic_target = Critic(obs_dim=obs_dim, action_dim=action_dim)\n",
    "        self.actor_target = Actor(obs_dim=obs_dim, action_dim=action_dim, action_low=env.action_space.low, action_high=env.action_space.high)\n",
    "\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # Create ADAM optimizer for the Critic and Actor networks\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, num_episodes: int) -> EpisodeStats:\n",
    "        \"\"\"\n",
    "        Train the DDPG agent.\n",
    "\n",
    "        :param num_episodes: Number of episodes to train.\n",
    "        :returns: The episode statistics.\n",
    "        \"\"\"\n",
    "        # Keeps track of useful statistics\n",
    "        stats = EpisodeStats(\n",
    "            episode_lengths=np.zeros(num_episodes),\n",
    "            episode_rewards=np.zeros(num_episodes),\n",
    "        )\n",
    "        current_timestep = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Print out which episode we're on, useful for debugging.\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}')\n",
    "\n",
    "            # Reset the environment and get initial observation\n",
    "            obs, _ = self.env.reset()\n",
    "\n",
    "            for episode_time in itertools.count():\n",
    "                # Use the Actor to choose an action\n",
    "                action = self.actor_target(obs)\n",
    "\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Update statistics\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                stats.episode_lengths[i_episode] += 1\n",
    "\n",
    "                # Store sample in the replay buffer\n",
    "                self.replay_buffer.store(obs, action, reward, next_obs, terminated)\n",
    "\n",
    "                # Sample a mini batch from the replay buffer\n",
    "                # A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch)\n",
    "                samples = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "                # Update the Critic network\n",
    "                update_critic(self.critic, self.critic_target, self.critic_optimizer, self.actor_target, self.gamma,\n",
    "                              samples[0], samples[1], samples[2], samples[3], samples[4])\n",
    "\n",
    "                # Update the Actor network\n",
    "                update_actor(self.critic, self.actor, self.actor_optimizer, samples[0])\n",
    "\n",
    "                # Update the target networks (Critic and Actor) via Polyak Update\n",
    "                polyak_update(self.critic, self.critic_target, self.tau)\n",
    "                polyak_update(self.actor, self.actor_target, self.tau)\n",
    "\n",
    "                current_timestep += 1\n",
    "\n",
    "                # Check whether the episode is finished\n",
    "                if terminated or truncated or episode_time >= 500:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now, let's run our algorithm on a task in [MinAtar](https://github.com/kenjyoung/MinAtar). The following game ID's are available: SpaceInvaders-v1, Breakout-v1, Seaquest-v1, Asterix-v1 and Freeway-v1.\n",
    "Note, that the training can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, run `pip install gymnasium[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\simon\\Downloads\\DeepRL\\.conda\\Lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Choose your environment\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Print observation and action space infos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\simon\\Downloads\\DeepRL\\.conda\\Lib\\site-packages\\gymnasium\\envs\\registration.py:756\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[0;32m    759\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simon\\Downloads\\DeepRL\\.conda\\Lib\\site-packages\\gymnasium\\envs\\registration.py:545\u001b[0m, in \u001b[0;36mload_env_creator\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \n\u001b[0;32m    538\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 545\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\Users\\simon\\Downloads\\DeepRL\\.conda\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1126\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\simon\\Downloads\\DeepRL\\.conda\\Lib\\site-packages\\gymnasium\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32mc:\\Users\\simon\\Downloads\\DeepRL\\.conda\\Lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:25\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox2D is not installed, run `pip install gymnasium[box2d]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, run `pip install gymnasium[box2d]`"
     ]
    }
   ],
   "source": [
    "# Choose environment\n",
    "env = gym.make(\"LunarLander-v2\", continuous = True, render_mode=\"rgb_array\")\n",
    "# Print observation and action space infos\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "\n",
    "# Hyperparameters:\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "TAU = 0.005\n",
    "EXPLORATION_NOISE=0.1\n",
    "NUM_EPISODES = 1_000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# Train DDPG\n",
    "agent = DDPGAgent(\n",
    "    env,\n",
    "    exploration_noise=EXPLORATION_NOISE,\n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    tau=TAU,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    ")\n",
    "stats = agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving and loading the trained actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained actor\n",
    "torch.save(agent.actor, \"resources/ddpg_actor.pt\")\n",
    "\n",
    "# loading the trained actor\n",
    "loaded_actor = torch.load(\"resources/ddpg_actor.pt\")\n",
    "loaded_actor.eval()\n",
    "print(loaded_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window=20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\")\n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array).astype(np.uint8)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            action = policy(torch.as_tensor(obs, dtype=torch.float32)).cpu().numpy()\n",
    "\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "imgs = rendered_rollout(loaded_actor, env)\n",
    "save_rgb_animation(imgs, \"resources/trained.gif\")\n",
    "IImage(filename=\"resources/trained.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
